{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projets de fin de formation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veuillez soumettre les éléments suivants :\n",
    "1. **Liens GitHub** :\n",
    "   - Fournissez les liens vers vos dépôts GitHub contenant le code source de vos projets.\n",
    "   - Fichier Notebook Jupyter (.ipynb) contenant le code, les analyses, et les visualisations. (Si vous avez utilisé un notebook)\n",
    "   - Rapport de Méthodologie : rédigez un rapport détaillant la méthodologie de traitement des données, y compris les étapes de prétraitement, les choix de modèles, et les évaluations de performance. (rapport obligatoire pour ceux qui ne travailleront pas avec un notebook< Pou ceux qui travailleront dans un notebook, ce dernier représentera votre rapport, il faudra donc y mettre un maximun de détals)\n",
    "2. **Liens d'Hébergement** :\n",
    "   - Fournissez les liens vers les plateformes où vos applications sont hébergées. Assurez-vous que tous les liens sont actifs et que les fichiers sont correctement nommés et organisés. Bonne chance !\n",
    "----\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projet 1 : Données statistiques NBA\n",
    "\n",
    "### Contexte\n",
    "Le jeu de données contient les statistiques depuis 1946 de la NBA et de l'ABA pour :\n",
    "- Statistiques de la saison régulière du joueur\n",
    "- Les totaux de carrière des joueurs en saison régulière\n",
    "- Statistiques des joueurs en séries éliminatoires\n",
    "- Totaux de carrière des joueurs en playoffs\n",
    "- Statistiques du match des étoiles des joueurs\n",
    "- Statistiques de l'équipe en saison régulière\n",
    "- Historique complet de la draft\n",
    "- coaches_season.txt - records des entraîneurs de la nba par saison\n",
    "- coaches_career.txt - records d'entraîneurs en nba par carrière\n",
    "\n",
    "### Idée de projet\n",
    "1. Détection des valeurs aberrantes sur les joueurs ; trouver qui sont les joueurs exceptionnels.\n",
    "2. Prédire l'issue du match.\n",
    "\n",
    "### Dataset\n",
    "Lien: [NBA data](https://um6p-my.sharepoint.com/:u:/g/personal/mahouzonssou_akotenou_um6p_ma/EeVhSMH3fsRAgdnr7Px8sEQBYVDc10uFftBbOkgt9074Vg?e=PKHhas)\n",
    "\n",
    "### Instructions\n",
    "1. Importer et explorer le dataset.\n",
    "2. Prétraiter les données (nettoyage, gestion des valeurs manquantes).\n",
    "3. Analyser les statistiques des joueurs pour détecter les valeurs aberrantes.\n",
    "4. Entraîner un modèle pour prédire l'issue des matchs.\n",
    "5. Créer une application web pour permettre l'analyse des joueurs et la prédiction des matchs.\n",
    "6. Déployer l'application\n",
    "\n",
    "### Bonne chance!\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projet 2 : Prédiction de l'Activité Physique\n",
    "\n",
    "### Contexte\n",
    "L'ensemble de données PAMAP2 sur la surveillance de l'activité physique contient des informations relatives à 18 activités physiques différentes (telles que la marche, le cyclisme, le football, etc.), effectuées par 9 sujets. Chaque sujet portait 3 unités de mesure inertielle et un moniteur de fréquence cardiaque. Ce dataset peut être utilisé pour la reconnaissance des activités et l'estimation de l'intensité, tout en développant et en appliquant des algorithmes de traitement des données, de segmentation, d'extraction de caractéristiques et de classification.\n",
    "\n",
    "### Problème\n",
    "Vous devez développer un modèle de classification capable de prédire l'activité physique d'un sujet en fonction des mesures capturées par les capteurs. Le défi consiste à utiliser les données sensorielles brutes pour classifier les différentes activités physiques réalisées par les sujets.\n",
    "\n",
    "### Dataset\n",
    "Lien : [Ensemble de données PAMAP2](https://archive.ics.uci.edu/dataset/231/pamap2+physical+activity+monitoring)\n",
    "\n",
    "### Capteurs\n",
    "- **3 unités de mesure inertielle sans fil Colibri (IMU)** :\n",
    "  - Fréquence d'échantillonnage : 100Hz\n",
    "  - Position des capteurs :\n",
    "    - 1 IMU au-dessus du poignet sur le bras dominant\n",
    "    - 1 IMU sur la poitrine\n",
    "    - 1 IMU sur la cheville du côté dominant\n",
    "- **Moniteur de fréquence cardiaque** :\n",
    "  - Fréquence d'échantillonnage : ~9Hz\n",
    "\n",
    "### Protocole de Collecte des Données\n",
    "- Chaque sujet a suivi un protocole contenant 12 activités différentes. Le dossier « Protocole » contient ces enregistrements par sujet.\n",
    "- Certains sujets ont également effectué des activités optionnelles. Le dossier « Optionnel » contient ces enregistrements par sujet.\n",
    "\n",
    "### Fichiers de Données\n",
    "Les données sensorielles brutes se trouvent dans des fichiers texte séparés par des espaces (.dat), avec un fichier de données par sujet et par session (protocole ou optionnel). Les valeurs manquantes sont indiquées par `NaN`. Une ligne dans les fichiers de données correspond à une instance horodatée et étiquetée de données sensorielles. Les fichiers de données contiennent 54 colonnes : chaque ligne est constituée d'un horodatage, d'une étiquette d'activité (la vérité de base) et de 52 attributs de données sensorielles brutes.\n",
    "\n",
    "### Instructions\n",
    "1. **Importation et Exploration des Données** : Importez les fichiers de données au format `.dat` et explorez les informations contenues dans les fichiers.\n",
    "2. **Prétraitement des Données** : Nettoyez les données, gérez les valeurs manquantes, et ajustez les horodatages au format correct.\n",
    "3. **Segmentation et Extraction de Caractéristiques** : Segmentez les données en fenêtres temporelles pertinentes et extrayez des caractéristiques significatives des mesures des capteurs.\n",
    "4. **Entraînement du Modèle** : Utilisez des techniques de classification pour créer un modèle capable de prédire l'activité physique basée sur les mesures des capteurs.\n",
    "5. **Évaluation du Modèle** : Évaluez les performances du modèle en utilisant des métriques appropriées pour les tâches de classification.\n",
    "6. **Déploiement de l'Application** : Créez une interface pour permettre la prédiction des activités physiques en temps réel à partir de nouvelles données.\n",
    "7. **Publication des Résultats** : Documentez les résultats et soumettez les prévisions conformément aux spécifications.\n",
    "\n",
    "### Bonne chance !\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projet 3: Analyse des Sentiments des Avis Clients\n",
    "\n",
    "### Contexte\n",
    "Les avis clients sont une source précieuse d'informations pour les entreprises souhaitant améliorer leurs produits et services. Analyser les sentiments exprimés dans ces avis permet de comprendre la satisfaction des clients et d'identifier les domaines à améliorer.\n",
    "\n",
    "### Problème\n",
    "Vous devez développer un modèle d'apprentissage automatique pour analyser les sentiments des avis clients et déployer une application qui permet aux utilisateurs de soumettre des avis et d'obtenir une analyse des sentiments en temps réel.\n",
    "\n",
    "### Dataset\n",
    "Lien: [Client review](https://um6p-my.sharepoint.com/:u:/g/personal/mahouzonssou_akotenou_um6p_ma/EZCq4NTMu91NnbEKu29ssQcBontmc8bGkwnL5nF6QEWMgw?e=uVYA7g)\n",
    "\n",
    "Dans le dataset, vous trouverez des fichiers au format `.parquet`. Pour charger ces données avec `pandas`, vous pouvez utiliser le code suivant :\n",
    "```python\n",
    "df = pd.read_parquet('chemin/vers/le/fichier.parquet')\n",
    "````\n",
    "\n",
    "### Instructions\n",
    "1. Importer et explorer le dataset.\n",
    "2. Prétraiter les données (nettoyage, tokenization).\n",
    "3. Entraîner un modèle de classification des sentiments (par exemple, un modèle de classification de texte utilisant un RNN, BERT, etc.).\n",
    "4. Évaluer la performance du modèle.\n",
    "5. Créer une application web avec Streamlit ou Flask pour permettre l'analyse des sentiments des avis soumis par les utilisateurs.\n",
    "6. Déployer l'application\n",
    "\n",
    "### Bonne chance!\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projet 4: Prévision des Ventes au Détail(<span style=\"color: red;\">Difficile, Self-Learning et Curiosité</span>)\n",
    "\n",
    "### Contexte\n",
    "Les entreprises de vente au détail dépendent fortement des prévisions de ventes précises pour la gestion des stocks et la planification des ressources.\n",
    "\n",
    "### Problème\n",
    "Vous devez construire un modèle de prévision des ventes à partir des données historiques et déployer une application qui permet aux utilisateurs de prédire les ventes futures en fonction des données entrées.\n",
    "\n",
    "### Dataset\n",
    "Lien: [store sales data](./dataset/store-sales-time-series-forecasting.zip)\n",
    "\n",
    "### Instructions\n",
    "1. Importer et explorer le dataset.\n",
    "2. Prétraiter les données (gestion des valeurs manquantes, encodage des variables catégorielles, etc.).\n",
    "3. Construire et entraîner un modèle de prévision (par exemple, ARIMA, LSTM).\n",
    "4. Évaluer la performance du modèle.\n",
    "5. Créer une application web pour permettre la prédiction des ventes.\n",
    "6. Déployer l'application\n",
    "\n",
    "### Bonne chance!\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projet 5: Détection des Fraudes Bancaires\n",
    "\n",
    "### Contexte\n",
    "La détection des fraudes est cruciale pour les institutions financières afin de protéger les clients et minimiser les pertes financières.\n",
    "\n",
    "### Problème\n",
    "Vous devez développer un modèle pour détecter les transactions frauduleuses et déployer une application qui permet de soumettre des transactions pour évaluation.\n",
    "\n",
    "### Dataset\n",
    "Lien: [creditcardfraud data](./dataset/creditcardfraud.zip)\n",
    "\n",
    "### Instructions\n",
    "1. Importer et explorer le dataset.\n",
    "2. Prétraiter les données (normalisation, gestion des classes déséquilibrées).\n",
    "3. Entraîner un modèle de détection de fraudes (par exemple, Random Forest, SVM).\n",
    "4. Évaluer la performance du modèle.\n",
    "5. Créer une application web pour permettre la détection des fraudes.\n",
    "6. Déployer l'application\n",
    "\n",
    "### Bonne chance!\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projet 6: Prédiction du Prix de l'Immobilier\n",
    "\n",
    "### Contexte\n",
    "La prédiction des prix de l'immobilier est essentielle pour les agents immobiliers et les acheteurs potentiels afin de prendre des décisions informées.\n",
    "\n",
    "### Problème\n",
    "Vous devez développer un modèle pour prédire les prix de l'immobilier et déployer une application qui permet aux utilisateurs de soumettre des caractéristiques de propriétés pour obtenir une estimation du prix.\n",
    "\n",
    "### Dataset\n",
    "Lien: [house prices data](./dataset/house-prices-advanced-regression-techniques.zip)\n",
    "\n",
    "### Instructions\n",
    "1. Importer et explorer le dataset.\n",
    "2. Prétraiter les données (gestion des valeurs manquantes, encodage des variables catégorielles).\n",
    "3. Entraîner un modèle de prédiction des prix (par exemple, une régression linéaire, XGBoost).\n",
    "4. Évaluer la performance du modèle.\n",
    "5. Créer une application web pour permettre la prédiction des prix de l'immobilier.\n",
    "6. Déployer l'application\n",
    "\n",
    "### Bonne chance!\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projet 7 : Diagnostic Respiratoire avec l'E-Nose de la NASA (<span style=\"color: red;\">Très Difficile</span>)\n",
    "\n",
    "### Contexte\n",
    "La NASA, à travers sa Science Mission Directorate (SMD), cherche à améliorer la précision de son dispositif E-Nose en tant qu'outil clinique potentiel. Ce dispositif mesure la composition moléculaire de l'haleine humaine pour fournir des résultats diagnostiques. Le défi est de développer un modèle de classification capable de distinguer de manière précise les individus positifs et négatifs à la COVID-19 à partir des données obtenues lors d'une étude clinique récente.\n",
    "\n",
    "### Problème\n",
    "Vous devez développer un modèle de classification capable de diagnostiquer les patients atteints de COVID-19 en utilisant les données capturées de l'haleine des volontaires avec le dispositif E-Nose de la NASA. Le défi est accentué par la taille limitée de l'échantillon, avec seulement 63 patients. Il y a très peu de donnée. C'est ce qui complique ce challenge.\n",
    "\n",
    "### Dataset\n",
    "Lien : [Dataset E-Nose](./dataset/e-naza-dataset.zip)\n",
    "\n",
    "Les données ont été exposées au dispositif E-Nose pour tous les patients en utilisant des fenêtres d'exposition selon le processus suivant :\n",
    "1. Mesure de base de 5 minutes avec de l'air ambiant\n",
    "2. Exposition à un échantillon de souffle et mesure de 1 minute, en utilisant le sac de souffle rempli\n",
    "3. Récupération des capteurs de 2 minutes avec de l'air ambiant\n",
    "4. Exposition à un échantillon de souffle et mesure de 1 minute, en utilisant le sac de souffle rempli\n",
    "5. Récupération des capteurs de 2 minutes avec de l'air ambiant\n",
    "6. Exposition à un échantillon de souffle et mesure de 1 minute, en utilisant le sac de souffle rempli\n",
    "7. Récupération des capteurs de 2 minutes avec de l'air ambiant\n",
    "\n",
    "**Temps total = 14 minutes**\n",
    "\n",
    "Les données sont réparties en ensembles d'entraînement et de test :\n",
    "- Entraînement : 45 patients\n",
    "- Test : 18 patients\n",
    "\n",
    "\n",
    "### Instructions\n",
    "1. **Importation et Exploration des Données** : Importez les fichiers de données et explorez les informations contenues dans les fichiers TXT et CSV.\n",
    "2. **Prétraitement des Données** : Nettoyez les données, gérez les valeurs manquantes, et ajustez les timestamps au format correct.\n",
    "3. **Analyse des Données** : Étudiez les mesures des capteurs et les annotations pour développer des caractéristiques pertinentes pour la classification.\n",
    "4. **Entraînement du Modèle** : Utilisez des techniques avancées de préparation des données et des modèles d'apprentissage automatique pour créer un modèle de classification.\n",
    "5. **Évaluation du Modèle** : Évaluez les performances du modèle en utilisant les ensembles de données d'entraînement et de test fournis.\n",
    "6. **Déploiement de l'Application** : Créez une interface web permettant d'entrer les données et de recevoir un diagnostic en temps réel.\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projet 8 : Analyse et prédiction de ventes d’un supermarché\n",
    "Contexte\n",
    "Ce jeu de données contient les ventes d’un supermarché réparties par villes, catégories de produits, sexe des clients, méthodes de paiement et autres informations contextuelles.\n",
    "\n",
    "### Idée de projet\n",
    "\n",
    "1. Analyse exploratoire des ventes et du comportement des clients.\n",
    "2. Prédire les ventes futures (régression).\n",
    "3. Pipeline MLOps léger pour automatiser l’entraînement.\n",
    "\n",
    "### Dataset\n",
    "Lien : https://www.kaggle.com/datasets/faresashraf1001/supermarket-sales\n",
    "\n",
    "### Instructions\n",
    "\n",
    "- Importer et explorer les données : tendances, pics, catégories populaires.\n",
    "- Nettoyer le dataset : outliers, encodage, normalisation.\n",
    "- Construire un modèle de prédiction des ventes.\n",
    "- Créer une petite API (FastAPI / Flask) pour interroger le modèle.\n",
    "- Déployer localement ou sur Render/railway.app ou HuggingFace Spaces.\n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projet 9 : Classification d’images de plantes (santé des feuilles)\n",
    "\n",
    "### Contexte\n",
    "Le dataset contient des images de feuilles saines ou atteintes de différentes maladies.\n",
    "\n",
    "### Idée de projet\n",
    "1. Détection de maladies via CNN.\n",
    "2. Dashboard simple pour charger une image et avoir le diagnostic.\n",
    "\n",
    "### Dataset\n",
    "Lien : https://www.kaggle.com/datasets/vipoooool/new-plant-diseases-dataset\n",
    "\n",
    "### Instructions\n",
    "1. Importer et visualiser quelques images du dataset.\n",
    "2. Construire un pipeline de prétraitement (augmentation d’images).\n",
    "3. Entraîner un CNN ou utiliser du Transfer Learning.\n",
    "4. Développer une application Streamlit pour uploader des images.\n",
    "5. Déployer l’app.\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projet 10 : Analyse de la qualité de l’air en Afrique\n",
    "### Contexte\n",
    "Le jeu de données contient les mesures quotidiennes de pollution (PM2.5, Ozone, etc.) dans différentes villes africaines.\n",
    "\n",
    "### Idée de projet\n",
    "1. Analyse des tendances et anomalies de pollution.\n",
    "2. Forecast des niveaux de pollution avec ARIMA ou LSTM.\n",
    "\n",
    "### Dataset\n",
    "Lien : https://www.kaggle.com/datasets/rohanrao/air-quality-data-in-india\n",
    " (peut être réutilisé car format similaire, ou remplacer par Afric AQ datasets disponibles)\n",
    "\n",
    "### Instructions\n",
    "1. Charger et nettoyer les données temporelles.\n",
    "2. Détecter les anomalies.\n",
    "3. Construire un modèle de prédiction du PM2.5.\n",
    "4. Déployer une API qui renvoie le PM2.5 prévu pour la semaine.\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projet 11 : Système de recommandation de films\n",
    "### Contexte\n",
    "Le dataset MovieLens 100k contient des milliers de films et des notes attribuées par les utilisateurs.\n",
    "\n",
    "### Idée de projet\n",
    "1. Analyse exploratoire des films les plus populaires.\n",
    "2. Système de recommandation collaboratif ou basé contenu.\n",
    "\n",
    "### Dataset\n",
    "Lien : https://grouplens.org/datasets/movielens/100k/\n",
    "\n",
    "### Instructions\n",
    "1. Importer les données et effectuer une analyse.\n",
    "2. Implémenter un système de recommandation simple (modèle basé sur la similarité).\n",
    "3. Construire une API pour recommander 5 films.\n",
    "4. Déployer l'app (Streamlit + API).\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projet 12 : Détection de fraudes bancaires\n",
    "### Contexte\n",
    "Dataset synthétique contenant des transactions frauduleuses et normales.\n",
    "\n",
    "### Idée de projet\n",
    "1. Analyse des transactions et des patterns suspects.\n",
    "2. Modèle de classification (Random Forest, XGBoost…).\n",
    "3. Mise en place d’un pipeline d’entraînement automatisé.\n",
    "\n",
    "### Dataset\n",
    "Lien : https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud\n",
    "\n",
    "### Instructions\n",
    "1. Importer et gérer le fort déséquilibre des classes.\n",
    "2. Construire plusieurs modèles pour comparer les performances.\n",
    "3. Déployer une API (FastAPI) pour détecter la fraude en temps réel.\n",
    "4. Intégrer un système de logs (MLOps).\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projet 13 : Analyse du trafic routier et détection d’embouteillages\n",
    "### Contexte\n",
    "Le dataset contient des capteurs placés sur des routes, donnant le flux de véhicules, la vitesse moyenne, etc.\n",
    "\n",
    "### Idée de projet\n",
    "1. Analyse des heures de pointe.\n",
    "2. Détection d’embouteillages via clustering.\n",
    "3. Prédiction du trafic.\n",
    "\n",
    "### Dataset\n",
    "Lien : https://www.kaggle.com/datasets/fedesoriano/traffic-prediction-dataset\n",
    "\n",
    "### Instructions\n",
    "1. Explorer et visualiser les datas temporelles.\n",
    "2. Détecter les patterns de congestions.\n",
    "3. Construire un modèle de prédiction (LSTM ou simple regression).\n",
    "4. Dashboard pour visualiser en temps réel les prédictions.\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projet 14 : Analyse et prédiction des prix immobiliers\n",
    "### Contexte\n",
    "Dataset contenant les prix de maisons, les caractéristiques des biens, la localisation, etc.\n",
    "\n",
    "### Idée de projet\n",
    "1. Analyse des facteurs influençant le prix.\n",
    "2. Modèle de régression des prix.\n",
    "3. Déploiement d’une app pour estimer le prix d’un bien.\n",
    "\n",
    "### Dataset\n",
    "Lien : https://www.kaggle.com/datasets/shree1992/housedata\n",
    "\n",
    "### Instructions\n",
    "1. Nettoyage, traitement des valeurs manquantes.\n",
    "2. Feature engineering (surface, âge, localisation).\n",
    "3. Modèle de prédiction + comparaison.\n",
    "4. Déploiement d’une interface utilisateur.\n",
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projet 15 : Pipeline Data Engineering + ML sur données d’e-commerce\n",
    "### Contexte\n",
    "Dataset contenant l’historique des commandes, produits, clients et interactions.\n",
    "\n",
    "### Idée de projet\n",
    "1. Création d’un pipeline ETL complet.\n",
    "2. Analyse des ventes et comportement client.\n",
    "3. Modèle de prédiction du taux d’abandon.\n",
    "\n",
    "### Dataset\n",
    "Lien : https://www.kaggle.com/datasets/carrie1/ecommerce-data\n",
    "\n",
    "### Instructions\n",
    "1. Construire un pipeline (ingestion → nettoyage → transformation).\n",
    "2. Stocker les données propres (parquet / CSV).\n",
    "3. Modèle pour classer les clients à risque.\n",
    "4. Orchestrer le pipeline avec Airflow / Prefect (optionnel).\n",
    "5. Déploiement modèle + dashboard.\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projet 16 : Analyse des réseaux sociaux (Twitter / X)\n",
    "### Contexte\n",
    "Dataset contenant des tweets, leurs textes, likes, retweets, sentiments, etc.\n",
    "\n",
    "### Idée de projet\n",
    "1. Analyse des tendances.\n",
    "2. Classification du sentiment (NLP).\n",
    "3. Dashboard de suivi en temps réel.\n",
    "\n",
    "### Dataset\n",
    "Lien : https://www.kaggle.com/datasets/kazanova/sentiment140\n",
    "\n",
    "### Instructions\n",
    "1. Nettoyage du texte (stopwords, stemming).\n",
    "2. Entraîner un modèle NLP (Logistic Regression, BERT…).\n",
    "3. Créer un dashboard avec les résultats.\n",
    "4. Déployer une API qui effectue la prédiction de sentiment.\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projet 17 : Système de prédiction de churn (désabonnement)\n",
    "### Contexte\n",
    "Dataset \"Telco Customer Churn\" contenant les caractéristiques des utilisateurs et s'ils ont quitté le service.\n",
    "\n",
    "### Idée de projet\n",
    "1. Analyse des clients à risque.\n",
    "2. Modèle de classification pour prédire le churn.\n",
    "3. Déploiement d’un mini-système d’alertes (MLOps).\n",
    "\n",
    "### Dataset\n",
    "Lien : https://www.kaggle.com/datasets/blastchar/telco-customer-churn\n",
    "\n",
    "### Instructions\n",
    "1. Nettoyage + encodage des variables catégorielles.\n",
    "2. Entraîner un modèle (Random Forest ou XGBoost).\n",
    "3. Créer une API + dashboard.\n",
    "4. Ajouter logs et monitoring.\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonne chance !\n",
    "<span style=\"color:blue; font-weight:bold\">**Vous pouvez y arriver !**</span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pygenomics",
   "language": "python",
   "name": "pygenomics"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
